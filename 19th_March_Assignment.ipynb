{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b85a5329-3dfc-4623-843b-21cf71454294",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dffd67-371e-4701-88c8-90d42d6ef15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max scaling, also known as normalization, is a popular technique used in data preprocessing to transform numeric features into \n",
    "# a common range.It rescales the values of a feature to a fixed range, usually between 0 and 1. The formula for Min-Max scaling is as \n",
    "# follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ae413e-34e4-4774-b51a-2d48964aa2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled_value = (value - min_value) / (max_value - min_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad09f4ac-57cd-48be-a4d1-29ea4b5d96d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this formula, \"value\" represents the original value of a data point, \"min_value\" is the minimum value of the feature in the dataset,\n",
    "# and \"max_value\" is the maximum value of the feature in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cdfc52-f701-4d4f-b92c-6a648d0281f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max scaling is often used when the features in a dataset have different scales, and it is desirable to bring them to a standardized \n",
    "# range.This preprocessing step is particularly useful for machine learning algorithms that rely on distance calculations or when working \n",
    "# with algorithms that require features to be on a similar scale to ensure fair comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec74a74-9603-48fc-ace3-593f0a73621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say we have a dataset with a feature representing the age of individuals, ranging from 18 to 80. We want to perform Min-Max \n",
    "# scaling on this feature. The minimum value (min_value) is 18, and the maximum value (max_value) is 80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0b2524-7834-4f24-a6e7-72f3f047fa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original values:\n",
    "\n",
    "# Individual A: 30\n",
    "# Individual B: 50\n",
    "# Individual C: 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f050ff6-e474-4375-bf83-639fb4ec3aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled values:\n",
    "\n",
    "# Individual A: (30 - 18) / (80 - 18) = 0.195\n",
    "# Individual B: (50 - 18) / (80 - 18) = 0.445\n",
    "# Individual C: (25 - 18) / (80 - 18) = 0.101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dabbc7b-19ba-40ab-9e27-f8cfa95ddd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After applying Min-Max scaling, the values are transformed to a common range between 0 and 1, allowing for better comparison and analysis \n",
    "# of the feature across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aa2ee0-37ce-45bc-8cb5-5480f52c643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that Min-Max scaling assumes a linear relationship between the original values and the scaled values. It can be sensitive to \n",
    "# outliers, as they can significantly affect the scaling range. Therefore, it's important to handle outliers appropriately before applying \n",
    "# Min-Max scaling if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7334781-2211-41ae-b726-98a1702082eb",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb7ce9c-5c4b-414a-87f0-122c6ff2eb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Unit Vector technique, also known as normalization or feature scaling, is a method used to rescale the magnitude of feature vectors \n",
    "# to have a unit norm or length of 1. Unlike Min-Max scaling, which brings the values of features within a specific range, Unit Vector \n",
    "# scaling focuses on the direction or orientation of the feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601b3397-b649-48db-99fb-40ef55e9b486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit_vector = feature_vector / ||feature_vector||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f718743e-f34e-483c-8aba-bc11ab37fdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, \"feature_vector\" represents the original vector, and ||feature_vector|| denotes the Euclidean norm (also known as L2 norm) of the \n",
    "# vector, which is the square root of the sum of the squared values of its components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa55d9b-4507-4585-98b1-1d9dab84ab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Vector scaling is often used when the magnitude or scale of the feature values is not important, but the direction or relative \n",
    "# importance of the features is significant. It is commonly applied in various machine learning algorithms, such as cosine similarity \n",
    "# calculations and clustering algorithms like K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21f7570-9a5d-430f-80e0-c12acae7dcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original feature vectors:\n",
    "\n",
    "# Individual A: [170, 65]\n",
    "# Individual B: [185, 80]\n",
    "# Individual C: [160, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437b33ba-5414-4333-8b65-e14579da44a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled feature vectors:\n",
    "\n",
    "# Individual A: [170/√(170² + 65²), 65/√(170² + 65²)] ≈ [0.930, 0.368]\n",
    "# Individual B: [185/√(185² + 80²), 80/√(185² + 80²)] ≈ [0.914, 0.407]\n",
    "# Individual C: [160/√(160² + 50²), 50/√(160² + 50²)] ≈ [0.932, 0.363]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f14de16-f820-424a-bcfb-1918c7b01fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After applying Unit Vector scaling, the length or norm of each feature vector becomes 1, while the direction or relative importance of \n",
    "# the features is maintained. This can be useful when you want to focus on the relationships between the features rather than their \n",
    "# absolute magnitudes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08911b78-a0b4-4365-8844-77f6cb745081",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8886a1ec-9904-44c0-97fa-94cbe97d7463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA (Principal Component Analysis) is a dimensionality reduction technique used to transform high-dimensional data into a lower-\n",
    "# dimensional space while retaining the most important information or patterns present in the data. It achieves this by identifying the \n",
    "# principal components, which are new variables that capture the maximum variance in the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd991e42-e909-44e8-98d8-334df1e4cb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The steps involved in performing PCA are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63538933-d2ed-46a6-8378-7b33379d7781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Standardize the data: If the features in the dataset have different scales, it is recommended to standardize them to have zero mean \n",
    "# and unit variance. This step ensures that all features contribute equally to the PCA analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffeb62a-47f8-4e85-9923-438d0e148495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Compute the covariance matrix: The covariance matrix is calculated based on the standardized data. It provides information about the \n",
    "# relationships between the features and is used to determine the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfa7961-375b-49b9-8468-e7730b456157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Compute the eigenvectors and eigenvalues: The eigenvectors and eigenvalues are obtained from the covariance matrix. The eigenvectors \n",
    "# represent the directions or components of maximum variance, while the eigenvalues indicate the amount of variance captured by each \n",
    "# eigenvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547fc804-e52f-4480-b16d-a88f8e548886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Sort the eigenvectors: The eigenvectors are sorted based on their corresponding eigenvalues in descending order. \n",
    "# This step determines the order of importance of the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb50b2d-978b-4c1f-a530-ea250d1bf5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Select the desired number of principal components: Depending on the desired dimensionality reduction, a certain number of principal \n",
    "# components are chosen. Typically, the top K components that capture the most significant variance are selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3983d90-5267-4961-ba2e-fefa7dad8daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Project the data onto the new feature space: The selected principal components are used as the basis to project the original data \n",
    "# onto the lower-dimensional space. This projection results in a transformed dataset with reduced dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b89bb6-45d5-400c-a319-d9e6ba0accc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. PCA is commonly used in various applications, such as data visualization, noise reduction, and feature extraction. It helps to \n",
    "# simplify the complexity of high-dimensional data, improve computational efficiency, and remove redundant or less important features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d56e9b-5be0-4dc4-8dec-7d8b77513393",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e4c9ac-45ac-4f7d-9076-119cb3d2cc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA (Principal Component Analysis) is a dimensionality reduction technique used to transform high-dimensional data into a lower-\n",
    "# dimensional space while retaining the most important information or patterns present in the data. It achieves this by identifying the \n",
    "# principal components, which are new variables that capture the maximum variance in the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f15343e-e65d-4a22-ac6f-d4e24e4c8060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider a dataset with 1000 images, each represented by a high-dimensional feature vector with 500 dimensions. These features might \n",
    "# correspond to pixel intensities or higher-level descriptors of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8788004-d095-473d-b958-622cb8d1661a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To reduce the dimensionality and extract important features using PCA, we can follow these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4861478a-e1e4-4dc7-861e-ebd518e1867b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Standardize the data: We standardize the feature vectors to have zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff603075-8b7a-46c0-8c03-f4c136860bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Compute the covariance matrix: Based on the standardized data, we calculate the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e64cf1-7c2a-4b5a-9d45-d70319831f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Compute the eigenvectors and eigenvalues: We compute the eigenvectors and eigenvalues from the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6306f59d-08d9-4fa8-8976-2d7af1600f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Sort the eigenvectors: We sort the eigenvectors based on their corresponding eigenvalues in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80c5f7d-3843-4a2f-88d3-b3722a377e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Select the desired number of principal components: Let's say we choose to select the top 50 principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5599f53-65c7-4e8e-ad28-aa3dfd9409c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Project the data onto the new feature space: We project the original feature vectors onto the space defined by the selected \n",
    "# principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741b6df7-9856-4aa1-b655-b21d2bbe48ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After applying PCA, we have transformed our original high-dimensional feature vectors into a lower-dimensional space consisting of 50 \n",
    "# principal components. These principal components capture the most important variations or patterns in the original data.\n",
    "# The reduced set of principal components can be used as the extracted features for further "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf845fd-b898-48b0-ac57-8238556f1739",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61b4d1c-cd53-4f0e-b4ef-763aa4ae8da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To preprocess the data for building a recommendation system for a food delivery service, you can utilize Min-Max scaling on certain \n",
    "# features such as price, rating, and delivery time. Here's how you can apply Min-Max scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234f1bb8-1fc0-4526-9948-0600e3fe8dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Understand the data: Analyze the range and distribution of the features you intend to scale, namely price, rating, and delivery time. \n",
    "# Determine the minimum and maximum values for each feature in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2bcb9d-edb8-43e4-9ffd-6b248b3abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Apply Min-Max scaling: Use the Min-Max scaling formula to scale the values of each feature to a common range between 0 and 1. \n",
    "# The formula is as follows: scaled_value = (value - min_value) / (max_value - min_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bff5da3-9371-4c56-b4a5-7bf2fff28268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Implement Min-Max scaling: For each feature (price, rating, and delivery time), apply the Min-Max scaling formula to transform \n",
    "# the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f700c84-a2be-4f37-afb2-5a319a6c4a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Interpret the scaled data: After scaling, the features will be rescaled to the range of 0 to 1. A value of 0 corresponds to the \n",
    "# minimum value of the feature in the dataset, while a value of 1 corresponds to the maximum value. The scaled values represent the \n",
    "# relative position of each data point within the range of the feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bb5a68-65fd-49f8-b1f5-0c7c1e8f63c4",
   "metadata": {},
   "source": [
    "# Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2d6663-1f52-4397-a879-e8639a9b755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To reduce the dimensionality of the dataset in order to build a model for predicting stock prices, you can employ PCA \n",
    "# (Principal Component Analysis). Here's how you can utilize PCA for dimensionality reduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af055bcc-588c-4cd7-83b0-753e98beef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data preprocessing: Ensure that your dataset is prepared appropriately by handling missing values, outliers, and standardizing \n",
    "# the features. It is important to standardize the features to have zero mean and unit variance, as PCA is sensitive to the scale of \n",
    "# the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d92f7e1-56c1-4eff-b248-bc2f0ffa2269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Perform PCA: Apply PCA to the standardized dataset to extract the principal components. PCA will transform the original features \n",
    "# into a new set of orthogonal variables known as principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae57e92-5a3b-441c-8422-2380b4eb9bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Determine the number of principal components: Analyze the cumulative explained variance ratio to determine the number of principal \n",
    "# components you wish to retain. The cumulative explained variance ratio depicts the amount of variance explained by each principal \n",
    "# component in decreasing order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed653ef2-d623-4129-9433-684de1e1ee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Select the desired number of principal components: Choose the number of principal components that capture a substantial amount of \n",
    "# variance in the data. You can base this decision on a predefined threshold, such as retaining components that collectively explain \n",
    "# 95% or more of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031341b9-c0f7-40a2-a6c5-ddb8137ab4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Transform the data: Project the original dataset onto the new feature space defined by the selected principal components. \n",
    "# This transformation yields a reduced-dimensional dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112205ba-52f4-4737-84a5-04d8ecc617da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Utilize the reduced dataset for modeling: Use the transformed, reduced-dimensional dataset for training your stock price \n",
    "# prediction model. The reduced dataset consists of the most important features that contribute significantly to the variance in the \n",
    "# original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47264b0-f299-4e88-8594-c6e133ed47aa",
   "metadata": {},
   "source": [
    "# Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75c9c6b-6fd0-4643-965d-70ef1dea03b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To perform Min-Max scaling on the dataset [1, 5, 10, 15, 20] and transform the values to a range of -1 to 1, follow these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2cbe18-2fc4-4d75-a4e0-1101aca8f98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the minimum and maximum values in the dataset:\n",
    "# Minimum value (min_value): 1\n",
    "# Maximum value (max_value): 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345db9f3-c684-42f8-a541-22cd74e3d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the Min-Max scaling formula to each value in the dataset:\n",
    "# scaled_value = 2 * (value - min_value) / (max_value - min_value) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d741b5-93e3-488f-8146-e9258d6453d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the scaled values:\n",
    "\n",
    "# For the value 1:\n",
    "# scaled_value = 2 * (1 - 1) / (20 - 1) - 1\n",
    "# = -1\n",
    "\n",
    "# For the value 5:\n",
    "# scaled_value = 2 * (5 - 1) / (20 - 1) - 1\n",
    "# = -0.6\n",
    "\n",
    "# For the value 10:\n",
    "# scaled_value = 2 * (10 - 1) / (20 - 1) - 1\n",
    "# = -0.2\n",
    "\n",
    "# For the value 15:\n",
    "# scaled_value = 2 * (15 - 1) / (20 - 1) - 1\n",
    "# = 0.2\n",
    "\n",
    "# For the value 20:\n",
    "# scaled_value = 2 * (20 - 1) / (20 - 1) - 1\n",
    "# = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c3727f-fce7-404d-a031-d70a1b021436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After applying Min-Max scaling, the dataset [1, 5, 10, 15, 20] is transformed to the range of -1 to 1 as follows:\n",
    "# [-1, -0.6, -0.2, 0.2, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d91fcf9-1aa7-43e3-984a-0f6cb67a1877",
   "metadata": {},
   "source": [
    "# Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c12eb3d-e375-4f0a-8d76-77b81745afac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To perform feature extraction using PCA on a dataset with features [height, weight, age, gender, blood pressure], we can follow \n",
    "# these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837eb7e4-a90d-41a8-8343-89942e8e73e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Standardize the data: Standardize the features in the dataset to have zero mean and unit variance. This step is crucial as \n",
    "# PCA is sensitive to the scale of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2cdfd3-2666-4b85-af92-b797da242325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Compute the covariance matrix: Calculate the covariance matrix based on the standardized dataset. The covariance matrix provides \n",
    "# information about the relationships and variances among the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cfc81f-a392-4f88-be45-6762074d75a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Compute the eigenvectors and eigenvalues: Compute the eigenvectors and eigenvalues from the covariance matrix. The eigenvectors \n",
    "# represent the principal components, and the eigenvalues indicate the amount of variance captured by each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a408d1fc-7892-4895-a040-4e9e4f9dce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Sort the eigenvectors: Sort the eigenvectors in descending order based on their corresponding eigenvalues. This step determines \n",
    "# the order of importance of the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957d30b1-911d-4ad8-99aa-48784367a671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Determine the number of principal components: Analyze the explained variance ratio or scree plot to decide the number of principal \n",
    "# components to retain. The explained variance ratio represents the proportion of the total variance explained by each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7617d89-c097-43c0-aba7-83679bf594e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a84d833-13da-4372-b036-6a8c5de100a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e35e9e-c74d-4e1d-b8c5-eed9a29798dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ef5bdb-8772-4f41-8de9-d8af15cc79af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
