{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "627984d4-731a-4c5c-9b61-33268784bd68",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b56b70-1124-4cb5-a002-f9a9a4f66c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In linear regression, the concept of R-squared (or coefficient of determination) is a statistical measure that assesses the \n",
    "# goodness-of-fit of a regression model. It indicates the proportion of the variance in the dependent variable that can be explained \n",
    "# by the independent variables in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd22b00-2eca-4599-af00-f8007c704069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-squared is calculated by squaring the correlation coefficient (r) between the observed values of the dependent variable and the \n",
    "# predicted values from the regression model. Mathematically, it can be represented as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d26548-d1ab-41e6-9fa1-fb09bf3383c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a step-by-step process to calculate R-squared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc332a8-6fca-4e5a-ab10-8d17489b153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fit the linear regression model to the data and obtain the predicted values (Y_hat) for the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9326da20-9cc5-49fa-80ee-d697d300f0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Calculate the mean of the observed values of the dependent variable (Y_bar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb346e7f-66ed-4bbd-9b8a-2074b409dac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Calculate the sum of squares total (SST), which represents the total variation in the observed dependent variable values:\n",
    "# SST = Σ((Y_i - Y_bar)^2)\n",
    "# where Y_i is the observed value of the dependent variable for each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f5e928-d389-47e2-96b3-286c0948a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Calculate the sum of squares regression (SSR), which represents the variation in the dependent variable explained by the regression \n",
    "# model:\n",
    "# SSR = Σ((Y_hat_i - Y_bar)^2)\n",
    "# where Y_hat_i is the predicted value of the dependent variable for each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0f594f-b04f-43ca-b899-02e9be871481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Calculate the sum of squares residual (SSE), which represents the unexplained variation or residual error:\n",
    "# SSE = Σ((Y_i - Y_hat_i)^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24564ea6-d07b-470c-a6c2-4045ee750ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Finally, calculate R-squared as the proportion of the total variation in the dependent variable that is explained by the \n",
    "# regression model:\n",
    "# R-squared = 1 - (SSE / SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7e2f1f-836c-43d7-9e23-bd00f7505fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-squared ranges from 0 to 1, where 0 indicates that the model explains none of the variance in the dependent variable, and 1 indicates \n",
    "# that the model explains all of the variance. Intermediate values represent the proportion of variance explained. However, it's important \n",
    "# to note that R-squared alone does not indicate the validity or significance of the model. It is necessary to consider other factors such\n",
    "# as p-values, confidence intervals, and residual analysis to evaluate the model's overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fac4b71-f0e5-4160-8f94-ca818636db91",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aafa21-cb8f-488a-aca1-0d80e71e2cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted R-squared is a modified version of R-squared that takes into account the number of predictors (independent variables) in a \n",
    "# regression model. While R-squared measures the proportion of variance in the dependent variable explained by the predictors, adjusted \n",
    "# R-squared adjusts this value based on the number of predictors and the sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49ccd41-79bc-405c-a450-3d1136e2b77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The formula to calculate adjusted R-squared is:\n",
    "# Adjusted R-squared = 1 - [(1 - R-squared) * (n - 1) / (n - k - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85de6a3-43a3-496a-93e6-98f18192423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where:\n",
    "# R-squared is the regular coefficient of determination.\n",
    "# n is the sample size.\n",
    "# k is the number of predictors (independent variables) in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651a5a88-6b4d-4dec-87e5-85d445aacb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's how adjusted R-squared differs from regular R-squared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef2cf01-02f1-4460-b949-c83580998d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Penalty for adding predictors: Regular R-squared tends to increase with the addition of predictors, even if they are insignificant \n",
    "# or do not contribute meaningfully to the model. Adjusted R-squared, on the other hand, adjusts for the number of predictors in the model. \n",
    "# It penalizes the addition of irrelevant predictors by decreasing the value of adjusted R-squared if the predictors do not improve the \n",
    "# model significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4e31ed-0d5b-4630-8940-057ffbce8c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Model complexity: Regular R-squared can be biased towards complex models with more predictors. It tends to increase as more \n",
    "# predictors are added, regardless of whether they truly improve the model's performance. Adjusted R-squared accounts for model complexity \n",
    "# by adjusting the R-squared value based on the sample size and the number of predictors. It provides a more accurate assessment of the \n",
    "# model's goodness-of-fit by considering both the explanatory power and the complexity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200cd129-6cb5-482f-8163-c2ea56c4e66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Interpretation: Regular R-squared is straightforward to interpret since it represents the proportion of variance explained by the \n",
    "# predictors. Adjusted R-squared provides a similar interpretation but takes into account the model's complexity. A higher adjusted R-\n",
    "# squared indicates a better fit of the model, considering the trade-off between explanatory power and the number of predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4eeae1-88eb-4df3-8613-dd453dc65508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall, adjusted R-squared is a useful metric to evaluate and compare regression models with different numbers of predictors. It helps \n",
    "# in selecting the most appropriate model by considering both the explanatory power and the complexity of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b680a807-13b0-49a5-9974-f3d3711bc740",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f02830-9c80-4e5a-a762-8c767b636bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted R-squared is more appropriate to use when comparing and evaluating regression models with different numbers of predictors. \n",
    "# It helps address the issue of model complexity and provides a more accurate assessment of the model's goodness-of-fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d6c25-4e26-4c79-8418-bfdc64688775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are some situations where adjusted R-squared is particularly useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf923a3-11a0-4f73-be4c-c1b7d33a6d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Model comparison: When comparing multiple regression models with different numbers of predictors, adjusted R-squared helps in \n",
    "# identifying the model that strikes a balance between explanatory power and model complexity. Models with higher adjusted R-squared values\n",
    "# are generally preferred as they explain more variance in the dependent variable while considering the number of predictors used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bec7cf8-e2d9-47ba-a00c-e28bc1a844ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Variable selection: Adjusted R-squared aids in variable selection by penalizing the inclusion of irrelevant or redundant predictors. \n",
    "# When building a regression model, adjusted R-squared can be used to assess the impact of adding or removing predictors. It helps in \n",
    "# determining whether the inclusion of additional predictors significantly improves the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc46076-6bda-4e87-84ab-dfd55cd0a587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Overfitting detection: Adjusted R-squared is particularly useful in detecting overfitting, which occurs when a model fits the \n",
    "# training data too closely but fails to generalize well to new data. If the regular R-squared increases as predictors are added, while \n",
    "# adjusted R-squared decreases or remains relatively stable, it suggests that the additional predictors may not be adding meaningful \n",
    "# explanatory power and might be overfitting the model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986eb9ee-2e89-464d-a8eb-8e17f613ab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Sample size consideration: Adjusted R-squared takes into account the sample size (n) and the number of predictors (k) in the model. \n",
    "# It is especially valuable when working with smaller sample sizes. Regular R-squared tends to be overly optimistic in such cases, \n",
    "# inflating the apparent goodness-of-fit. Adjusted R-squared adjusts for the sample size and provides a more reliable assessment of the \n",
    "# model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be655406-476d-4fa6-8b20-d3c03ef22cca",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a7049e-4c0d-4815-952e-f5c6fde00c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE (Root Mean Squared Error): RMSE is a measure of the average deviation between the predicted values and the actual values in a regression \n",
    "# model. It is calculated by taking the square root of the average of the squared differences between the predicted and actual values. RMSE is \n",
    "# useful for penalizing large errors, as the squared differences amplify the impact of larger errors. Lower RMSE values indicate better model \n",
    "# performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3c0380-c9b6-45f5-a7d6-c854259c63b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE (Mean Squared Error):MSE is similar to RMSE but without taking the square root. It is calculated by averaging the squared differences between \n",
    "# the predicted and actual values. Like RMSE, MSE also penalizes larger errors more heavily. Higher MSE values indicate worse model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f165e5cc-9f09-478a-b708-6891521d0dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE (Mean Absolute Error): MAE is another measure of the average deviation between the predicted and actual values in a regression model. \n",
    "#  Unlike RMSE and MSE, MAE calculates the average absolute differences between the predicted and actual values. MAE does not penalize large errors \n",
    "# as heavily as RMSE and MSE do since it does not involve squaring the differences. Lower MAE values indicate better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37756d49-e997-42ee-ba71-96cc2d814ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE = sqrt( (1/n) * sum( (predicted[i] - actual[i])^2 ) )\n",
    "# MSE = (1/n) * sum( (predicted[i] - actual[i])^2 )\n",
    "# MAE = (1/n) * sum( |predicted[i] - actual[i]| )\n",
    "# n is the number of data points in the dataset.\n",
    "# predicted[i] is the predicted value for the i-th data point.\n",
    "# actual[i] is the actual value for the i-th data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369f23fa-5c0c-4edd-b8bd-79d5276a0578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In summary, RMSE, MSE, and MAE are regression metrics that help quantify the accuracy of predictive models by measuring the deviations between\n",
    "# predicted and actual values. RMSE and MSE both penalize larger errors more heavily, while MAE provides a more balanced view by considering the \n",
    "# absolute differences. The choice of which metric to use depends on the specific requirements and preferences of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f521c102-58db-4b7a-9fdc-028660f3c4c3",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72d6bf6-9764-4039-8f8c-1958ac7b0f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advantages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89bbccd-8d2a-4bf2-9d9e-0af07e032aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Easy interpretation: All three metrics provide intuitive and easy-to-understand measures of the prediction accuracy. Lower values indicate \n",
    "# better model performance, and they can be easily compared across different models or variations of a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b902ac-4976-46df-995d-620726435afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Sensitivity to errors: RMSE and MSE are sensitive to large errors due to their squared nature. This can be advantageous when large errors \n",
    "# need to be penalized more heavily. For example, in certain applications like financial forecasting or anomaly detection, it may be crucial to \n",
    "# minimize significant deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6e13ab-c834-4dea-b203-3fd64b6399b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Mathematical properties: RMSE and MSE are mathematically well-behaved, ensuring that they are non-negative and increase with increasing \n",
    "# prediction errors. This makes them suitable for optimization techniques that rely on differentiable loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eecdd71-caf0-4f6e-a5ce-cc62dfd3e365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disadvantages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3c920e-e569-47c0-be1e-8ed5c210c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Sensitivity to outliers: RMSE and MSE are highly sensitive to outliers since they square the differences between predicted and actual values. \n",
    "# Outliers with large errors can disproportionately affect the metrics, leading to inflated values. If the dataset contains a significant number of \n",
    "# outliers, these metrics may not accurately reflect the overall model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ffad27-3870-4e50-81f5-fabd8e8f4eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Lack of interpretability: While RMSE and MSE are useful for quantifying the prediction accuracy, they do not provide direct insights into the \n",
    "#  magnitude of the errors. For example, if the RMSE is 10, it doesn't convey whether the model is making errors of 10 units or 100 units. MAE, on \n",
    "# the other hand, provides the average magnitude of the errors and can be more interpretable in this regard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8c13fe-b226-4cb5-b512-5744a6de005f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Scale dependence: RMSE and MSE are scale-dependent metrics since they involve squared differences. This means that the metric values can be \n",
    "# influenced by the scale of the target variable. Comparing RMSE or MSE values across models with different units or scales may not be meaningful \n",
    "# without proper normalization or standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc79232c-0ead-4801-baf8-2bec7d2ac5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Limited optimization: Unlike RMSE and MSE, MAE is not differentiable at zero, which limits its use in certain optimization algorithms that \n",
    "# rely on gradients. This can make it harder to optimize models using MAE as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be49eae-ca56-40be-9075-4d2cd0b644c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In practice, it is important to consider the specific characteristics of the dataset and the goals of the analysis when selecting the appropriate \n",
    "# evaluation metric. RMSE, MSE, and MAE each have their own advantages and disadvantages, and the choice should be based on the specific requirements \n",
    "# of the regression problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bb4504-d254-4342-87eb-15c1bd80b446",
   "metadata": {},
   "source": [
    "# Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e37c2-7d12-4ecd-87ba-5e814876575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso regularization, also known as L1 regularization, is a technique used in regression analysis to add a penalty term to the model's objective \n",
    "# function. It helps to prevent overfitting and improve the model's generalization by reducing the complexity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675a7560-dfc3-4ddd-b03f-42d50c511e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Lasso regularization, the penalty term is the sum of the absolute values of the coefficients multiplied by a regularization parameter (λ). \n",
    "# The objective function is modified by adding this penalty term, and the goal is to minimize the sum of the squared errors between the predicted \n",
    "# and actual values, along with the penalty term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f537b14a-5cca-414e-ad96-e42271425ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The key difference between Lasso regularization and Ridge regularization (L2 regularization) lies in the penalty term. In Ridge regularization, \n",
    "#the penalty term is the sum of the squared values of the coefficients, whereas in Lasso regularization, it is the sum of the absolute values of \n",
    "# the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12898ab7-38b9-44c7-8bd8-be57712f46b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to this difference, Lasso regularization has the property of performing feature selection by encouraging sparse solutions. It tends to drive \n",
    "# the coefficients of less important features to zero, effectively excluding them from the model. This makes Lasso regularization particularly useful \n",
    "# when dealing with high-dimensional datasets with many features, where feature selection and model interpretability are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd76506d-cb80-4d6b-8b94-8b3f264abfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When deciding between Lasso and Ridge regularization, it is essential to consider the characteristics of the problem and the specific goals of \n",
    "# the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9b8d15-bb75-46eb-bfc4-8b3d7461f613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Lasso regularization is generally preferred when there is a reason to believe that only a subset of the features is truly influential in the\n",
    "# model. It can help identify and focus on the most important predictors, reducing overfitting and improving interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c20107f-162d-41b5-97eb-6d324c550a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Ridge regularization, on the other hand, tends to shrink the coefficients towards zero without reducing them exactly to zero. It is more \n",
    "# suitable when dealing with datasets that have multicollinearity (high correlation) among predictors, as it can help stabilize the model and \n",
    "# mitigate the impact of correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad2c637-a268-4ef1-aed2-11814b5fc20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. If both feature selection and dealing with multicollinearity are concerns, Elastic Net regularization combines L1 (Lasso) and L2 (Ridge) \n",
    "# penalties, providing a balance between the two approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d28900d-1f42-4f5b-b207-db6d36ce3bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is worth noting that the choice between Lasso, Ridge, or Elastic Net regularization depends on the specific dataset and problem at hand. \n",
    "# Cross-validation and parameter tuning techniques can be employed to select the best regularization technique and fine-tune the regularization \n",
    "# parameter (λ)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b1c47b-1213-4d27-a9d6-e727a045828a",
   "metadata": {},
   "source": [
    "# Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e19e818b-2442-4a0a-87d8-f7b8b9af4c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized linear models help prevent overfitting in machine learning by introducing a penalty term to the model's objective function, which \n",
    "# discourages complex models with large coefficients. This penalty term encourages the model to find a balance between minimizing the errors on \n",
    "# the training data and reducing the magnitude of the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1471cdc-1ce4-425e-9759-fda6d53e36fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To illustrate this, let's consider an example of fitting a linear regression model with regularization. Suppose we have a dataset with one \n",
    "# input variable (X) and one target variable (y). We want to fit a linear regression model to predict y based on X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0806d1-5fb9-45ca-bcfb-54be700543eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without regularization, a simple linear regression model tries to find the best-fit line that minimizes the sum of squared errors between the\n",
    "# predicted values and the actual values in the training data. However, without constraints, the model can become too complex and sensitive to \n",
    "# noise, leading to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c1ff89-d842-4e3f-afa1-ce16816c305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's introduce regularization, specifically Ridge regularization (L2 regularization), to the linear regression model. Ridge regularization\n",
    "# adds a penalty term to the objective function, which is the sum of the squared values of the coefficients multiplied by a regularization parameter \n",
    "# (λ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816437c9-f49c-452e-b514-627f561a74fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By including this penalty term, the model is incentivized to minimize the squared errors while also keeping the coefficients small. This helps \n",
    "# to prevent overfitting by shrinking the coefficients, reducing their impact on the model's predictions. The regularization parameter (λ) controls \n",
    "# the strength of the penalty and balances the trade-off between the goodness of fit and the complexity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7f1a44-b825-489c-a2c8-cc24ac31f738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In practice, higher values of λ result in more regularization, leading to smaller coefficients and a simpler model. Lower values of λ reduce the \n",
    "# regularization effect, allowing the model to fit the training data more closely but potentially increasing the risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcfb966-339d-439e-ad72-23f56f2d59ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized linear models provide a systematic way to control the complexity of the model and prevent overfitting. By adding the penalty term, \n",
    "# they help strike a balance between fitting the training data well and generalizing to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d65b483-f40d-4944-b6ba-21b86f7ff4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's important to note that the example provided above demonstrates Ridge regularization (L2 regularization). Other regularization techniques, \n",
    "# such as Lasso regularization (L1 regularization) or Elastic Net regularization, have different penalty terms and achieve slightly different \n",
    "# effects in preventing overfitting. The specific choice of regularization technique depends on the problem and the characteristics of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce090f44-4318-4dfa-aeeb-a174807fb10d",
   "metadata": {},
   "source": [
    "# Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d31c71b-456f-4330-8670-c4630c8202f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# While regularized linear models offer several benefits in regression analysis, they also have limitations that make them not always the best \n",
    "# choice for every scenario. Let's discuss some of these limitations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e346c9b-422d-46da-8743-d5f0bf88162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Linear relationship assumption: Regularized linear models assume a linear relationship between the predictors and the target variable. \n",
    "# If the underlying relationship is highly nonlinear, linear models may not capture the complexity of the data accurately. In such cases, more \n",
    "# flexible nonlinear models, such as decision trees or neural networks, may be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffe7ab9-80d4-427a-a5b9-06a360967df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Feature interpretability: Regularized linear models tend to shrink coefficients towards zero, which can be advantageous for feature selection \n",
    "# and model interpretability. However, in certain cases where maintaining the interpretability of individual features is crucial, other techniques \n",
    "# that preserve the original scale and interpretation of features, such as decision trees or linear models without regularization, may be preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51e6449-ed0d-489b-b72d-c1ad2dba5fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Sensitivity to outliers: While regularization helps reduce the impact of outliers, it may not completely eliminate their influence. \n",
    "# Regularized linear models still assign non-zero coefficients to influential outliers, which can affect the model's predictions. In situations \n",
    "# where the presence of outliers is significant and their impact needs to be minimized, robust regression methods or outlier detection techniques \n",
    "# may be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e285d6-b1f2-470c-a35f-5d6b326adfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Over-regularization: Regularization introduces a bias towards simpler models by shrinking coefficients. However, in some cases, when the \n",
    "# signal-to-noise ratio in the data is high or when the number of predictors is small, over-regularization can lead to underfitting. In such \n",
    "# situations, non-regularized linear models or models with less aggressive regularization may yield better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b35ff8-6242-4f10-8264-2b9677006244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Limited flexibility: Regularized linear models have a fixed functional form and assume linearity. They may struggle to capture complex \n",
    "# interactions and nonlinear relationships between predictors, limiting their predictive power in certain scenarios. More flexible models, such as \n",
    "# polynomial regression or non-linear models, may be more suitable when the data exhibits nonlinear patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f1cdbd-b1e3-4f11-838f-7fb7c221f1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Hyperparameter tuning: Regularized linear models introduce additional hyperparameters, such as the regularization parameter (λ) in Ridge or \n",
    "# Lasso regression. Selecting an appropriate value for these hyperparameters requires careful tuning, which can be time-consuming and computationally \n",
    "# expensive. Improper selection of hyperparameters may lead to suboptimal model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ecb8bf-7e0e-48a3-8ec1-b03659b2eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In summary, while regularized linear models have proven to be effective in many regression analysis tasks, they are not universally suitable for \n",
    "# all situations. It is essential to consider the assumptions of linearity, interpretability requirements, sensitivity to outliers, and flexibility \n",
    "# needed in the model before choosing a regularized linear model. Exploring alternative techniques and understanding the specific characteristics of \n",
    "# the data can help determine the most appropriate modeling approach for a given regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adef17fc-18db-47bb-ac96-613e897a4a6e",
   "metadata": {},
   "source": [
    "# Q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f218647b-67a2-4a41-a5c7-f26d0b205b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the given scenario, Model A has an RMSE (Root Mean Squared Error) of 10, while Model B has an MAE (Mean Absolute Error) of 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936f1af5-2412-43de-acc1-43c99710ecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE and MAE are different metrics with distinct characteristics. RMSE penalizes larger errors more heavily due to the squared differences, while \n",
    "# MAE provides a more balanced view by considering the absolute differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a416e8b-5459-46b8-907d-9aae5a366877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we prioritize penalizing larger errors more heavily, we may lean towards Model A with an RMSE of 10. RMSE is sensitive to larger errors, \n",
    "# so if minimizing significant deviations is crucial in the problem domain, Model A might be preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe44879c-1417-4876-95fe-fb086b9378dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On the other hand, if we focus on the average magnitude of errors without overemphasizing the impact of outliers or large errors, we may prefer \n",
    "# Model B with an MAE of 8. MAE provides a straightforward interpretation of the average absolute difference between predicted and actual values, \n",
    "# which can be useful when the magnitude of errors is more important than their squared values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4379762e-ed01-47b5-985a-21792642fc7a",
   "metadata": {},
   "source": [
    "# Q10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3591fad4-36df-4da1-860d-ad2c9777c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the given scenario, Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with \n",
    "# a regularization parameter of 0.5. To determine which model is the better performer, we need to consider the characteristics of the regularization \n",
    "# methods and the specific context of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630a64f0-9fd6-4337-ba84-69a3c5f84761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aa7402-9cf9-4730-9bbf-2c4e8750a852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Ridge regularization adds a penalty term proportional to the sum of the squared values of the coefficients.\n",
    "# 2. It encourages smaller but non-zero coefficients, reducing the impact of less important predictors.\n",
    "# 3. Ridge regularization is effective in dealing with multicollinearity (high correlation) among predictors.\n",
    "# 4. It tends to perform well when the dataset has many predictors, and the goal is to stabilize the model and prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a94ad1-1788-4aaa-a356-411b58a74780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d96579-9e34-400b-9229-cf195a789a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Lasso regularization adds a penalty term proportional to the sum of the absolute values of the coefficients.\n",
    "# 2. It encourages sparsity in the coefficients, driving some coefficients to exactly zero, effectively performing feature selection.\n",
    "# 3. Lasso regularization is useful when the dataset has many predictors and only a subset of them is expected to be truly influential.\n",
    "# 4. It can lead to a more interpretable model by explicitly excluding irrelevant predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc159318-74ab-4358-aa85-9f6aec2e0628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To choose the better performer between Model A and Model B, we need to evaluate their performance based on specific requirements and priorities. \n",
    "# The decision can be based on various factors, such as prediction accuracy, interpretability, or the importance of feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb67feb1-44e4-431a-b84a-00b9b4286639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, the choice of regularization method is not without trade-offs and limitations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55601d51-afd8-42ac-9390-7f57c4e952c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In summary, the decision between Model A and Model B depends on the specific context and requirements of the problem. Ridge regularization is \n",
    "# suitable for stabilizing the model and handling multicollinearity, while Lasso regularization can provide feature selection and interpretability \n",
    "# benefits. Consideration of the trade-offs and limitations associated with each regularization method is crucial when selecting the better performer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bc2e7c-9ae7-4145-94b7-7284edc2488f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
