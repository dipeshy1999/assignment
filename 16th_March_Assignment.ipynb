{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b37129a0-cc2e-4c14-83cf-6ace3e0338c7",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4331d2a1-ec66-476a-926a-d408b5137c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In machine learning, overfitting and underfitting are common problems that occur when training a model on a given dataset. Both issues \n",
    "# relate to the model's inability to generalize well to new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95604398-b79b-43da-a8e7-652be5e0ec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting happens when a machine learning model performs exceptionally well on the training data but fails to generalize accurately on new, \n",
    "# unseen data. The model essentially \"memorizes\" the training data instead of learning the underlying patterns and relationships. Overfitting \n",
    "# can occur when a model becomes overly complex and captures noise or random fluctuations in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50758419-5ae1-4336-94de-8bee1e4f0e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consequences of overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4a1b51-dec0-44fa-a04e-7b30ef0e6de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poor performance on new, unseen data: The model fails to generalize well, leading to inaccurate predictions or classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b5b3b5-9613-45a6-925f-25564a1b5ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity to noise: Overfit models may be overly sensitive to small variations or noise in the input data, making them less robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f04a68-6611-4e79-93b8-459599cc4197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mitigation of overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c389e1-4ce3-423f-b1b3-4b55e48c375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase training data: Providing more diverse and representative training data can help the model learn general patterns rather than \n",
    "# memorizing specific instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca08a4e-aaad-43ef-b41e-d067decd5cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce model complexity: Simplify the model architecture, reduce the number of features, or use regularization techniques to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f09c0bb-9faf-4342-b2a6-89b7272f8506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation: Employ techniques like k-fold cross-validation to evaluate the model's performance on multiple subsets of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f4f680-bed2-4f5a-afd3-b81c552a1a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Underfitting occurs when a machine learning model fails to capture the underlying patterns and relationships in the training data. \n",
    "# The model is too simple or lacks the necessary complexity to adequately represent the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b558e98-7ef2-45b3-b4c2-6a53203de361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consequences of underfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f990a850-b2dc-4bfe-85b1-91a405a53179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inability to learn from the data: The model fails to capture the essential patterns and produces poor results on both the training and unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52a2b7c-4a88-463b-b356-a6369bb8d700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High bias: Underfit models typically exhibit high bias, meaning they have limited expressive power and struggle to capture complex relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b1bf0f-08c8-4fc4-a1bd-fdca02e47e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mitigation of underfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a74e15-b224-4f03-8c8c-2a270edd2da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase model complexity: Use a more sophisticated model with higher capacity, such as increasing the number of layers in a neural network \n",
    "# or using a more complex algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fef11c-f68b-48b4-a093-ca7e152cb17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering: Extract more relevant features or transform existing features to make the data more informative for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937eac4e-e9dc-4146-9a9c-2b74fed7e006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust hyperparameters: Increase the number of iterations, adjust learning rate, or modify other hyperparameters to allow the model more \n",
    "# capacity to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7a6983-baef-414d-8a95-fcc6c5012658",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab01830-6c4a-4844-b27b-8892954b6514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mitigation of overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f101f557-2f4f-49fc-80b1-d6a5887e23cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Increase training data: Providing more diverse and representative training data can help the model learn general patterns rather than \n",
    "# memorizing specific instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d33222-aaeb-4c90-8231-0dc5d7ee3300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Reduce model complexity: Simplify the model architecture, reduce the number of features, or use regularization techniques to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab976526-e2af-4921-a981-e664e82baa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Cross-validation: Employ techniques like k-fold cross-validation to evaluate the model's performance on multiple subsets of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897ec281-7912-4204-ac05-dd735c373a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Early stopping: Monitor the model's performance on a validation set during training and stop training when the performance starts to degrade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58441abe-5ed0-4a76-affb-8247949167f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Regularization: Add regularization terms, such as L1 or L2 regularization, to the loss function to penalize complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a53416f-dc02-4a51-b6e8-7746faf36349",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d2159b-28fa-4cb1-a7c1-1e94c88178a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Underfitting occurs when a machine learning model fails to capture the underlying patterns and relationships in the training data. \n",
    "# The model is too simple or lacks the necessary complexity to adequately represent the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24284cb0-e3a7-42e2-8f46-67d030cab0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Insufficient model complexity: If the chosen model is too simple or lacks the necessary capacity to capture the underlying patterns in \n",
    "# the data, it may result in underfitting. For example, using a linear model to represent a nonlinear relationship in the data can lead to \n",
    "# underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a57f612-d0d9-410d-8de1-b62fc942f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Limited training data: When the available training data is small or unrepresentative of the underlying distribution, the model may not \n",
    "# have enough information to learn the patterns accurately. Insufficient data can lead to underfitting as the model fails to capture the \n",
    "# complexity of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ea66a5-26f2-4fe4-b554-5e12d1b8cda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Incorrect feature selection: If the selected features do not adequately represent the underlying relationships in the data, the model may \n",
    "# struggle to learn effectively. Inadequate feature selection can result in underfitting, as the model lacks the necessary information to make \n",
    "# accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d196bb-4cce-4495-9d31-f3988ee3c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Early stopping: While early stopping can help prevent overfitting, it can also lead to underfitting if the model stops training too early. \n",
    "# If the training process is halted before the model has converged or learned the important patterns in the data, it may underfit the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7cb94a-80cc-46e1-8b1d-f1bed5e0152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Over-regularization: Regularization techniques, such as L1 or L2 regularization, are commonly used to prevent overfitting. However, if \n",
    "# the regularization strength is set too high, it can excessively constrain the model's learning capacity, leading to underfitting. \n",
    "# It is crucial to find the right balance between regularization and model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e48ac0-f95a-486e-9b83-13def0a6bb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Biased dataset: When the training data is biased or imbalanced, meaning some classes or instances are underrepresented, the model may \n",
    "# struggle to learn the minority classes or make accurate predictions for rare instances. This can result in underfitting, particularly for the \n",
    "# underrepresented classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e2d073-d623-4b35-b9c2-9657f2231223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Noisy data: If the training data contains significant noise or outliers, the model may struggle to capture the underlying patterns accurately. \n",
    "# Noisy data can introduce misleading information, making it difficult for the model to generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e19a3f0-f33b-4697-babc-bb2febcbeaff",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f920db8b-6af0-429a-8b90-1abe695b9e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The bias-variance tradeoff is a fundamental concept in machine learning that deals with the relationship between bias and variance and their \n",
    "# impact on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6b3970-8a10-43d8-a592-1f699140c4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias refers to the error introduced by approximating a complex real-world problem with a simplified model. It represents the model's tendency \n",
    "# to make overly simplistic assumptions about the data, leading to systematic errors. A high-bias model is typically too simple and cannot capture \n",
    "# the true underlying patterns in the data. It results in underfitting, where the model fails to learn the training data well and also performs\n",
    "# poorly on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7daf70b-be42-47ee-8702-6d679c87fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance, on the other hand, refers to the variability of the model's predictions across different training sets. It represents the model's \n",
    "# sensitivity to fluctuations or noise in the training data. A high-variance model is overly complex and excessively adapts to the noise in the \n",
    "# training data, leading to overfitting. Such a model performs exceptionally well on the training data but fails to generalize to new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa06cbb-db43-4d1b-8d89-89c37431230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The relationship between bias and variance can be summarized as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0564e01-5538-49a7-b55c-67f4a25f8b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High bias, low variance: Models with high bias tend to be too simple and make strong assumptions about the data. They have limited flexibility \n",
    "# and capacity to capture complex patterns. As a result, they exhibit consistent errors and have low variance across different training sets. \n",
    "# These models tend to underfit the data and may perform poorly both on the training data and new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0208c89-88b6-4f01-bacb-d101fc67633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low bias, high variance: Models with low bias are more complex and have higher flexibility to capture intricate relationships in the data. \n",
    "# They adapt well to the training data and exhibit low training error. However, they are highly sensitive to noise and random fluctuations, \n",
    "# leading to high variance across different training sets. These models tend to overfit the data, performing exceptionally well on the training \n",
    "# data but having poor generalization ability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd792c58-4c6f-488c-aabb-302ec2707102",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341be856-9d7f-40ba-802a-87881535586b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting overfitting and underfitting in machine learning models is essential for understanding model performance and making necessary \n",
    "# adjustments. Here are some common methods for detecting these issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f095715-6aa7-4771-b1c4-66a0829fdd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics: Analyzing the performance metrics on both the training and validation/test datasets can provide insights into potential \n",
    "# overfitting or underfitting. If the model shows significantly better performance on the training data compared to the validation/test data, \n",
    "# it indicates overfitting. Conversely, if the model performs poorly on both training and validation/test data, it suggests underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46706040-77d5-462e-8d91-c87570b4bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curves: Plotting learning curves that show the model's performance (e.g., accuracy, loss) as a function of training iterations or \n",
    "# data size can help identify overfitting or underfitting. In an overfit model, the training error will decrease over time, while the validation \n",
    "# error may start increasing or plateau. An underfit model will have high training and validation errors that don't converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74433bb-dbe6-42a0-a05d-f91aae87c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation: Utilizing techniques like k-fold cross-validation provides a robust way to assess the model's performance. If the model \n",
    "# performs well across different folds (splits) of the data, it indicates a good generalization ability. However, if there are significant \n",
    "# performance variations among the folds, it suggests overfitting or underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8faa2e-be66-4c7c-896c-edc5ace6ce67",
   "metadata": {},
   "source": [
    "# Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cf6671-a4f0-496d-ab67-3eb55b484e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias and variance are two sources of error in machine learning models that affect their performance and ability to generalize. \n",
    "# Here's a comparison between bias and variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73a6ea1-0e23-4472-bf97-97574641f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc69600-e57a-48a8-99a8-ee5a23c81ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias refers to the error introduced by approximating a complex problem with a simplified model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee71bfd-8b23-49f1-a4f1-0cf335eb9b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High bias models are overly simplistic and make strong assumptions about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2906573-b512-4c69-b29d-d96ea936fb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These models have limited capacity to capture complex patterns and relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01df9bd-5f50-4d59-9ef7-d3c2594bce3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  High bias leads to underfitting, where the model performs poorly on both the training and test/validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60f524b-b302-4c88-be88-00d85c98041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of high bias models: Linear regression with few features, a decision tree with limited depth, or a linear classifier when \n",
    "# the data exhibits complex non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406b97a7-7131-4d5d-8e6a-496144e0051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808ba2f5-8d97-4f6a-b07a-efdbda94da3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance refers to the variability of the model's predictions across different training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfc6dd1-3f40-4e04-a0b3-d4522887a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High variance models are highly flexible and can capture intricate details in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abd7009-7be8-4529-acd3-952d49a106b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These models tend to overfit the training data, becoming excessively sensitive to noise or random fluctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403c5ee7-8a5f-4f9b-a2c3-80ebccea6691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High variance leads to poor generalization, where the model performs well on the training data but poorly on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9fde09-78f8-4339-8bc8-fa27cb172f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of high variance models: Decision trees with deep branching, high-degree polynomial regression, or neural networks with a \n",
    "# large number of layers and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecae027-5270-4c4a-8283-107e9d9eac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differences in performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c763592-aada-47d4-bf69-7b60df8f027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High bias models have a tendency to oversimplify the data, leading to consistent but often inaccurate predictions. They exhibit a high error \n",
    "# rate and have low complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46abae9b-9aa9-430e-8ad8-03e85ebb0f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High variance models have a greater capacity to fit the training data, resulting in low training error. However, they are highly sensitive \n",
    "# to noise and have poor generalization, leading to high error rates on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48b9127-b441-4f3a-ac3a-1a3501658d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High bias models perform poorly both on the training and test/validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c969884-3ca8-41e9-b6dd-de325b92a2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High variance models may perform exceptionally well on the training data but fail to generalize to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354eb59f-245f-4ed8-b2bf-33ec90ec30a1",
   "metadata": {},
   "source": [
    "# Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edefbb3-535b-43a4-a669-09a05a9a9717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization in machine learning is a technique used to prevent overfitting by adding a penalty term to the model's objective function. \n",
    "# The penalty term discourages the model from excessively fitting the training data and encourages it to generalize better to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf5ed6d-759a-4b89-b41f-45099703a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common regularization techniques include:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5c966c-2eb1-45d0-b46b-5d5b31dec3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. L1 Regularization (Lasso):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981e27c6-3cae-42e0-837e-90ded7591656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 regularization adds the sum of the absolute values of the model's coefficients as the penalty term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ce4f4-4aff-4ac8-a7fd-4b8b8eae5fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It promotes sparsity by encouraging some coefficients to become exactly zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9012ba-1634-4534-b221-8b89093a82da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helps in feature selection by shrinking less important features to zero, effectively reducing the model's complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b148975a-6874-4d7a-9954-10cadfa4f89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 Regularization (Ridge):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317cb903-5d6a-4bee-acad-61b562a03d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 regularization adds the sum of the squared values of the model's coefficients as the penalty term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e1186e-22aa-40bf-9015-d84095f182e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It encourages small but non-zero coefficients for all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c57b1-695b-4c79-99c7-e12cf73357be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This technique helps in reducing the impact of individual features without eliminating them completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0e7c47-a69f-42ec-9808-834664430ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Elastic Net Regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2854091-cd85-41a3-a8eb-beecb407196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic Net regularization combines L1 and L2 regularization by adding a linear combination of both penalty terms to the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca7b000-ce05-4f47-b8c9-642a113ad863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It provides a balance between feature selection (sparsity) and regularization (shrinking coefficients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8bf3b5-4c4c-4923-bc77-c3eedb221be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9b16a4-b808-4f47-b60a-cc15ca4bfe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout is a regularization technique commonly used in neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f465325-0add-4e72-b990-7e9f61286f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# During training, it randomly sets a fraction of the neuron activations to zero at each training iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cfd4c8-3390-4348-b3bc-22619b2d4569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This prevents the model from relying too much on specific neurons and encourages the network to learn more robust and generalizable representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e91686b-ff0c-41ae-b32e-1d72bfe6a8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Early Stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a3efa1-8c6a-4638-96f7-846b9c5761f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping is a simple regularization technique that monitors the model's performance on a validation set during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a049e57-9da5-4fa2-9a2f-8bd4172f61b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training is stopped when the validation performance starts to deteriorate, preventing the model from overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a845d304-158b-4ff6-bca2-9b111b1d7fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It allows the model to be trained for the optimal number of iterations, avoiding unnecessary training that leads to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3525664e-461d-411e-ab2e-e9b593e72456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization techniques work by introducing a penalty term that modifies the objective function the model optimizes. \n",
    "# This penalty term controls the complexity of the model and influences the weights or coefficients assigned to different features. \n",
    "# By adding this regularization term, the model is discouraged from fitting the noise in the training data and instead focuses on the \n",
    "# more meaningful patterns, leading to improved generalization and reduced overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e716f964-17a2-4720-baa2-241ebe0a681b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedc97c9-a385-473b-abf0-6dc84c10341c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
