{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6123feca-a4bd-4802-ab04-3c9502341270",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5031124e-7958-4705-abc0-8d2f5a5c4baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear regression is a statistical technique used to model the relationship between a single independent variable and a \n",
    "# dependent variable. It assumes a linear relationship between the independent and dependent variables, meaning that the change in the \n",
    "# dependent variable is directly proportional to the change in the independent variable. The objective of simple linear regression is to \n",
    "# find the best-fit line that minimizes the sum of squared differences between the observed and predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84012829-70da-4626-b2df-7349d1185ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Simple Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ee0817-fef4-4dc0-8033-7c3dc5d5eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say you want to predict a student's final exam score based on the number of hours they studied. Here, the number of hours studied \n",
    "# (independent variable) is used to predict the final exam score (dependent variable). The simple linear regression model will estimate \n",
    "# the relationship between these two variables by fitting a straight line to the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20047124-973b-4262-8c1e-1f7ac15b449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple linear regression is an extension of simple linear regression that allows for the analysis of the relationship between a \n",
    "# dependent variable and multiple independent variables. It assumes a linear relationship between the dependent variable and each \n",
    "# independent variable, while also considering the potential interaction effects among the independent variables. The objective of \n",
    "# multiple linear regression is to find the best-fit line that minimizes the sum of squared differences between the observed and \n",
    "# predicted values, taking into account multiple independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33347fcb-560c-4246-8b5d-e885aa600090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Multiple Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e847c6d0-8077-4fb7-aae3-23fa1e468e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you want to predict a house's sale price based on various factors such as its size, number of bedrooms, and location. \n",
    "# Here, the sale price (dependent variable) is influenced by multiple independent variables, including size, number of bedrooms, and \n",
    "# location. Multiple linear regression can be used to estimate the relationship between the sale price and these independent variables, \n",
    "# considering their combined effects on the house's value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010ac9b3-1c3c-444d-a156-e931b5dbd0a3",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6158502-1c95-4d99-887e-38b0325b88ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression relies on several assumptions to ensure the validity and reliability of its results. Here are the key assumptions \n",
    "# of linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122be445-1604-4f36-ba59-abcaa9f5b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Linearity: The relationship between the independent and dependent variables is assumed to be linear. This means that the change in \n",
    "# the dependent variable is directly proportional to the change in the independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a73fef-7789-4f97-b501-aaf7ead322c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Independence: The observations in the dataset are assumed to be independent of each other. In other words, there should be no \n",
    "# correlation or relationship between the residuals (the differences between the observed and predicted values) of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24455cab-a3ca-4134-9f8d-b9878a32924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Homoscedasticity: Homoscedasticity assumes that the variance of the residuals is constant across all levels of the independent \n",
    "# variables. This means that the spread or dispersion of the residuals should remain constant as the values of the independent variables \n",
    "# change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e66389-748f-4d6e-ab6b-c700f5b27375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normality: The residuals are assumed to follow a normal distribution. This assumption is necessary to perform hypothesis tests, \n",
    "# construct confidence intervals, and make reliable predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eb0498-ddbf-46a8-ab58-781bcb433be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No multicollinearity: In multiple linear regression, the independent variables should not be highly correlated with each other. \n",
    "# Multicollinearity can lead to unstable and unreliable estimates of the regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c525f2e-2067-4603-a663-896319ae7822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check whether these assumptions hold in a given dataset, you can perform several diagnostic tests and visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67c97b0-1daf-462c-9bc8-efa23fd56e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Residual analysis: Plot the residuals against the predicted values to check for patterns or non-linearity. A random scatter of \n",
    "# residuals around zero indicates linearity. Additionally, inspecting the residuals for any patterns over time or across independent \n",
    "# variables can provide insights into the assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b751f7-1beb-4dc3-9ebf-a06d89c1d4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Normality test: Plot a histogram or a Q-Q plot of the residuals to assess their distribution. Alternatively, statistical tests such \n",
    "# as the Shapiro-Wilk test or the Kolmogorov-Smirnov test can be used to formally test the normality assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc669c8-12d0-4544-9633-744357caedd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Homoscedasticity test: Plot the residuals against the predicted values or the independent variables to check for a constant spread. \n",
    "# If there is a funnel-like pattern or a systematic change in the spread, it suggests heteroscedasticity. Various statistical tests, \n",
    "# such as the Breusch-Pagan test or the White test, can also be conducted to assess heteroscedasticity formally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e83f73-2c74-430a-944b-305f630bb5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Multicollinearity assessment: Calculate the correlation matrix among the independent variables. If there are high correlations \n",
    "# (above a certain threshold, e.g., 0.7 or 0.8), it indicates potential multicollinearity. Additionally, variance inflation factor (VIF) \n",
    "# analysis can be performed to quantify the extent of multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7e4aae-5771-410f-b7dc-2d282351fdc1",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73339f20-a077-4109-b646-179a82b197e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a linear regression model, the slope and intercept represent the relationship between the independent variable(s) and the dependent \n",
    "# variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2dfb11-1d2a-480c-b5dc-ac5127e5502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretation of the Slope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec804e0a-24ce-4aff-8762-44745e64de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The slope (also known as the regression coefficient) indicates the change in the dependent variable for a one-unit change in the \n",
    "# independent variable, while holding other variables constant. It represents the average rate of change in the dependent variable \n",
    "# associated with a unit change in the independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d6fb91-1bd2-4913-9e80-6a06923f6981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, let's consider a linear regression model that predicts a person's salary (dependent variable) based on their years of \n",
    "# experience (independent variable). If the slope is estimated to be 5000, it means that, on average, for each additional year of \n",
    "# experience, the person's salary is expected to increase by $5000, assuming all other factors remain constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6433a1-b6e8-4bd9-a754-a56fecf6adb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretation of the Intercept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc596a7-dc21-4c29-b97c-e5b4b0b4e21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The intercept (also known as the constant term) represents the estimated value of the dependent variable when all independent \n",
    "# variables are zero. It provides the starting point or baseline value of the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c144e23a-422d-443e-a32e-099aa6abc595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuing with the previous example, suppose the intercept of the salary regression model is estimated to be $30,000. This implies \n",
    "# that a person with zero years of experience would have an estimated salary of $30,000. The intercept accounts for the baseline salary \n",
    "# that is not explained by the independent variable(s) in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66e43c6-2af8-442f-8c49-5b25bcc48136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's important to note that the interpretation of the slope and intercept may vary depending on the specific context and the units of \n",
    "# the variables involved. Additionally, in multiple linear regression, the interpretation of the slope coefficients can be influenced by\n",
    "# the presence of other independent variables in the model, as they account for the joint effect of all variables included.\n",
    "# In summary, the slope represents the change in the dependent variable associated with a one-unit change in the independent variable, \n",
    "# while the intercept represents the estimated value of the dependent variable when all independent variables are zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf4c866-14d9-4a25-8081-8ed60e02b17c",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9130813e-9dbd-4dd8-b5bb-ceb610603e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent is an optimization algorithm that is used to find the minimum of a function. It works by starting at a point and then moving \n",
    "# in the direction of the steepest descent until it reaches a minimum. The direction of the steepest descent is the negative of the gradient of \n",
    "# the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2d0b96-1ea8-481e-be9b-52f7c1fc0387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In machine learning, gradient descent is used to train models. The model is represented by a set of parameters, and the goal is to find the \n",
    "# values of these parameters that minimize the cost function. The cost function is a measure of how well the model fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7095cc93-d7bf-41b7-9a82-ffe67ab4cb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent works by iteratively updating the parameters of the model in the direction of the negative gradient of the cost function. \n",
    "# This means that the model is always moving towards a better fit to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc5b6b2-c763-4c98-a4f7-6d53e43a8891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are different variants of gradient descent, such as batch gradient descent, stochastic gradient descent, and mini-batch gradient descent. \n",
    "# The choice of which variant to use depends on the specific problem being solved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e8e828-2cba-41eb-b50a-02e6c3882fa0",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90085a17-48e0-4c73-8402-d1e688758b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ure. Multiple linear regression is a statistical model that predicts a dependent variable based on two or more independent variables. \n",
    "# The model is a linear equation that takes the following form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925c7a08-389a-45b9-9e1b-9727e2649b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = b0 + b1x1 + b2x2 + ... + bpxp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b1bedf-2299-4c19-b16b-b792bb60fc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y is the dependent variable\n",
    "# b0 is the intercept\n",
    "# b1, b2, ..., bp are the coefficients of the independent variables\n",
    "# x1, x2, ..., xp are the independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82fbaef-7606-40f1-a691-328e7a1d4001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model is fit to the data by minimizing the sum of squared errors between the predicted values and the actual values of the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e2648-ccfa-4205-97a3-eca285092008",
   "metadata": {},
   "source": [
    "# Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ae91ed-6363-44e6-af5f-0d6beaf87ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sure. Multicollinearity is a phenomenon in multiple linear regression where two or more independent variables are highly correlated. \n",
    "# This means that the independent variables are redundant and provide essentially the same information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757c313f-fa0c-4d93-995d-910ae97f33ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multicollinearity can cause problems in multiple linear regression, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46fb8b3-b958-4d3b-abbc-d85ea51f2dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The standard errors of the coefficients may be inflated, making it difficult to determine the statistical significance of the coefficients.\n",
    "# The coefficient estimates may be unstable, meaning that they may change significantly if the model is refit with a different sample of data.\n",
    "# The model may not be able to predict the dependent variable accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5123f9-f2b2-47b7-b783-5d47dbcdc476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a few ways to detect multicollinearity. One way is to look at the correlation matrix of the independent variables. If two or more \n",
    "# independent variables have a high correlation coefficient, then there is likely to be multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c021ae5-0976-4054-81b5-1b3ccdd4887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to detect multicollinearity is to use the variance inflation factor (VIF). The VIF is a measure of how much the variance of a \n",
    "# coefficient is inflated due to multicollinearity. A VIF of 1 indicates that there is no multicollinearity, while a VIF of 10 or higher indicates \n",
    "# that there is significant multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289681b3-d0c7-4dbc-ab42-c920169180f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a few ways to address multicollinearity. One way is to remove one of the correlated independent variables. This can be done by \n",
    "# looking at the correlation matrix and removing the independent variable with the highest correlation with the other independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c8d93d-03ea-4c86-af5f-5d06422a944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to address multicollinearity is to combine the correlated independent variables into a single composite variable. This can be done \n",
    "# by creating a new variable that is the sum or average of the correlated independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b5dfdc-20eb-4596-8ef6-7af7a7231a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, it is also possible to use a technique called partial least squares regression to address multicollinearity. Partial least squares \n",
    "# regression is a method of regression that takes into account the correlation between the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf59b09-46e9-4be9-950e-a854ef9f76df",
   "metadata": {},
   "source": [
    "# Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb835931-a388-452c-9619-d845c7c833df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial regression is a type of regression analysis in which the relationship between the independent variable x and the dependent variable y \n",
    "# is modeled as an nth degree polynomial function. In other words, instead of fitting a straight line (as in linear regression), a curve fits the \n",
    "# data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186fc419-7b25-4e80-b1c6-2ec14d565346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The polynomial regression model is a generalization of the linear regression model. In linear regression, the relationship between x and y is \n",
    "# modeled as a linear function, which means that it can be represented by a straight line. In polynomial regression, the relationship between x \n",
    "# and y can be represented by a curve of any degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd93e3e-1578-4db2-b70f-82c909cff0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main difference between polynomial regression and linear regression is that polynomial regression can model non-linear relationships between \n",
    "# x and y, while linear regression can only model linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1e66b3-c913-4394-941b-c9a76c98cf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is an example of how polynomial regression can be used. Let's say we want to predict the height of a child based on their age. We could use \n",
    "# polynomial regression to fit a model to the data and predict the height of a child given their age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a361791b-b0c9-42a0-ae65-5d3acda40e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model would be fit to the data by minimizing the sum of squared errors between the predicted heights and the actual heights of the children. \n",
    "# The coefficients of the model would tell us how much each term in the polynomial function contributes to the prediction of the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7bb260-05a7-4eb6-8aa5-79af1eac0198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, if the coefficient for the age term is positive, then we know that age is positively correlated with height. This means that as a \n",
    "# child's age increases, their height is also likely to increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3ca7f4-2ee7-4fbe-8081-caaf82fa0fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial regression is a powerful tool that can be used to predict a wide variety of variables. It is a versatile model that can be used in \n",
    "# many different applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78de509-e5b4-4c01-9456-750d284257f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature\t                                        Polynomial Regression\t                                      Linear Regression  \n",
    "# Relationship between x and y                       Can be non-linear                                               Must be linear\n",
    "# Model\t                                             Polynomial function\t                                         Linear function\n",
    "# Coefficients                           Tell us how much each term in the polynomial           Tell us how much each independent variable contributes \n",
    "#                                        function contributes to the prediction of the            to the prediction of the dependent variable\n",
    "#                                        dependent variable\n",
    "# Applications                           Can be used to predict a wide variety of variables       Typically used to predict variables that have \n",
    "#                                                                                                 linear relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ef4a0d-79f1-4192-b3b8-dcb7963e7f67",
   "metadata": {},
   "source": [
    "# Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf314ec9-8acb-46eb-9754-aadf2b6290ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advantages of polynomial regression over linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cf8200-599a-48e5-b082-28f7fedd5a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial regression can model non-linear relationships, while linear regression can only model linear relationships.\n",
    "# Polynomial regression can be more accurate than linear regression for data that has a non-linear relationship.\n",
    "# Polynomial regression is more flexible than linear regression, which means that it can be used to fit a wider variety of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc1f8f2-6303-4f02-9277-7fd56428c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disadvantages of polynomial regression over linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbabf9b-c27d-403f-9178-498e65ca18e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial regression can be more complex than linear regression, which can make it more difficult to interpret the results.\n",
    "# Polynomial regression can be more sensitive to outliers than linear regression.\n",
    "# Polynomial regression can overfit the data, which means that it can fit the training data too well and not generalize well to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b62e0de-4402-439c-acd1-d86842f1ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial regression is a good choice when the relationship between the independent and dependent variables is non-linear. For example, \n",
    "# if you want to predict the height of a child based on their age, you would use polynomial regression because the relationship between height \n",
    "# and age is non-linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fdd2d7-42c8-45b6-be1a-6018260289da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial regression is also a good choice when you need a more flexible model than linear regression. For example, if you want to predict \n",
    "# the price of a house based on its square footage, number of bedrooms, and number of bathrooms, you would use polynomial regression because the \n",
    "# relationship between price and these variables is likely to be non-linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb601561-4eef-4230-851a-a7ce0284753d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
