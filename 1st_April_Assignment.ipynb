{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b170dfc-489f-4ff9-ba13-eddba4dccb4c",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb642cc-eda3-4b08-8f6a-82a270682437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression and logistic regression are both popular statistical models used in machine learning, but they are suited for different types \\\n",
    "# of problems and data. Here's a breakdown of their differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a322d0-24ee-42c2-b135-9f0c45131bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Objective: Linear regression is used for predicting continuous numeric values, whereas logistic regression is used for predicting binary \n",
    "# categorical outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223ceece-0aa3-4a49-b8bd-f762b1693a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Output: Linear regression models produce a continuous output, typically represented as a numeric value. The predicted output can range from \n",
    "# negative infinity to positive infinity. On the other hand, logistic regression models produce a probability value between 0 and 1, representing \n",
    "# the likelihood of a binary outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b698c8-d2f0-4b89-ac73-e1c999a1df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Assumption: Linear regression assumes a linear relationship between the independent variables (inputs) and the dependent variable (output). \n",
    "# It assumes that the dependent variable can be expressed as a linear combination of the independent variables, with some added random noise. \n",
    "# Logistic regression, however, assumes a logit (logistic) transformation of the linear combination of inputs to model the relationship between the \n",
    "# variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc31b15-29fc-41b0-a716-1fe284e6f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Model Type: Linear regression is a regression model, as it predicts a continuous outcome. It can be represented by a straight line (simple \n",
    "# linear regression) or a hyperplane (multiple linear regression) in a multidimensional space. Logistic regression, on the other hand, is a \n",
    "# classification model, as it predicts the probability of a binary outcome. It uses a logistic function (S-shaped curve) to map the linear \n",
    "# combination of inputs to a probability value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0241e5-7625-4724-ba14-19dca09ce3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Loss Function: Linear regression uses a loss function like Mean Squared Error (MSE) to measure the difference between the predicted and actual \n",
    "# continuous values. Logistic regression employs a loss function called Log Loss or Cross-Entropy Loss, which calculates the dissimilarity between \n",
    "# the predicted probabilities and the actual binary labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f24cb4-91c9-43ae-a380-e657498724b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's consider an example scenario where logistic regression would be more appropriate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c98693-cd2c-4881-b261-432c581ee6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you are working on a project to predict whether a customer will churn (cancel their subscription) or not in a subscription-based service, \n",
    "# such as a telecom company. Here, the outcome of interest is binary (churned or not churned). Logistic regression would be suitable for this \n",
    "# problem because it can estimate the probability of churn based on various customer attributes (e.g., age, usage patterns, customer tenure, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b4857-50d4-4193-96c8-1712db97d88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using logistic regression, you can build a model that takes these customer attributes as inputs and predicts the probability of churn. By setting \n",
    "# a threshold, you can classify customers as either likely to churn (probability above the threshold) or unlikely to churn (probability below the \n",
    "# threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f210d00-7e72-4e07-b9b0-de2d1a747fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case, linear regression would not be appropriate since it is designed for predicting continuous values, and it would not naturally \n",
    "# provide the binary churn/non-churn predictions needed in this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41305b4-ee90-41fa-815f-1af21ef97883",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0973252-024c-4b9e-b8b5-a0bcb3a73170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cost function used in logistic regression is called the Log Loss or Cross-Entropy Loss. It measures the dissimilarity between the predicted \n",
    "# probabilities and the actual binary labels. The goal is to minimize this cost function during the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4d9994-05ab-449c-ab75-b2b5782d06ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assume we have a binary classification problem with two classes: 0 and 1. Given a training set with m examples, the cost function for \n",
    "# logistic regression is defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdea553d-8d0c-42a7-ace5-dd512e439963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost(hθ(x), y) = -y * log(hθ(x)) - (1 - y) * log(1 - hθ(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8795231c-5749-4337-afeb-2a7335caf2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the above equation:\n",
    "# hθ(x) represents the predicted probability that the output (y) is 1 given the input (x) and the model's parameters (θ).\n",
    "# y is the actual binary label (0 or 1) of the training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3b048d-3903-4533-8f3a-2b033fa59afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To optimize the cost function and find the optimal parameters (θ) that minimize the cost, gradient descent or other optimization algorithms \n",
    "# are commonly used. The basic idea behind gradient descent is to iteratively update the parameters in the opposite direction of the gradient \n",
    "# (derivative) of the cost function with respect to the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a17adda-e5dc-4424-a0b5-343fb97c2120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The update rule for gradient descent in logistic regression is:\n",
    "# θ := θ - α * (∂J/∂θ)\n",
    "# Where:\n",
    "# α (alpha) is the learning rate, determining the step size in each iteration.\n",
    "# J is the cost function, which needs to be minimized.\n",
    "# (∂J/∂θ) represents the partial derivative of the cost function with respect to each parameter θ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34ac06c-e216-4575-a4b8-cbeadeed44a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By iteratively applying the gradient descent update rule, the parameters (θ) are adjusted in the direction that minimizes the cost function. \n",
    "# The process continues until convergence, where the cost function is minimized or reaches a predefined threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e736141b-913d-42f3-900a-ab2a72c1b7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, more advanced optimization algorithms like L-BFGS, conjugate gradient, or Newton's method can also be used to optimize the cost \n",
    "# function in logistic regression. These methods often converge faster than standard gradient descent but require more computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e57ddc-e2f0-4370-ab90-f53dd8c74d5a",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35db1e17-fe67-4117-b051-668d3e2f0f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization in logistic regression is a technique used to prevent overfitting by adding a penalty term to the cost function. Overfitting \n",
    "# occurs when a model becomes too complex and fits the training data too closely, resulting in poor generalization to new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84499721-44b8-49f6-ac5e-c8fbaff2cc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In logistic regression, regularization is typically applied using either L1 regularization (Lasso) or L2 regularization (Ridge). Both methods \n",
    "# introduce a regularization term that encourages the model to have smaller parameter values, thus reducing the complexity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055133f7-5b85-4822-b3e5-ebceba4f75c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 Regularization (Lasso):\n",
    "# In L1 regularization, the cost function is modified by adding the absolute values of the model's parameter values multiplied by a regularization \n",
    "# parameter (λ). The cost function with L1 regularization is given by:\n",
    "# Cost(hθ(x), y) = -y * log(hθ(x)) - (1 - y) * log(1 - hθ(x)) + λ * ∑|θ|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b114133-8012-488b-ae2e-d7dc118d996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The regularization term (∑|θ|) penalizes large parameter values, and the regularization parameter (λ) controls the strength of the regularization. \n",
    "# L1 regularization has the property of driving some parameter values to exactly zero, effectively performing feature selection by eliminating less\n",
    "# relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d04fd32-1cc0-40ce-a2bb-7137d46816e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 Regularization (Ridge):\n",
    "# In L2 regularization, the cost function is modified by adding the squared values of the model's parameter values multiplied by a regularization\n",
    "# parameter (λ). The cost function with L2 regularization is given by:\n",
    "# Cost(hθ(x), y) = -y * log(hθ(x)) - (1 - y) * log(1 - hθ(x)) + λ * ∑θ^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467083d1-560d-4fe3-9123-3146b0b24a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to L1 regularization, the regularization parameter (λ) controls the strength of the regularization. However, unlike L1 regularization, \n",
    "# L2 regularization does not lead to exact zeroing of parameter values. Instead, it encourages smaller but non-zero parameter values, effectively\n",
    "# shrinking the parameters towards zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a9dc0a-7805-4fca-85c1-d09c38b44bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How Regularization Prevents Overfitting:\n",
    "# Regularization helps prevent overfitting by adding a penalty to the cost function for having large parameter values. This penalty discourages \n",
    "# the model from relying too heavily on individual features or complex interactions between features, making it generalize better to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c01994-2530-4494-aa66-580acea50d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By controlling the regularization parameter (λ), the trade-off between model complexity and the goodness of fit can be adjusted. A larger λ value \n",
    "# increases the penalty on large parameter values, leading to a simpler model with potentially higher bias but lower variance. Conversely, a smaller \n",
    "# λ value reduces the regularization effect, allowing the model to capture more complex relationships but increasing the risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6b8833-ad7b-4b47-99a6-e878b5bd072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization, whether L1 or L2, encourages the model to find a balance between fitting the training data well and avoiding overfitting, \n",
    "# ultimately improving its performance on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285bc7b7-f6f0-4047-bbc2-f7e0046821cb",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963eb21c-cbcc-40c5-b481-49e9e7132829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ROC (Receiver Operating Characteristic) curve is a graphical representation that illustrates the performance of a binary classification model,\n",
    "# such as logistic regression. It displays the trade-off between the true positive rate (TPR) and the false positive rate (FPR) at different \n",
    "# classification thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf29072-ac35-4535-b392-e9f38276afe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To understand how the ROC curve is used to evaluate the performance of a logistic regression model, let's go through the following steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bae1b4-dcd5-4954-b345-6e5b3b58f5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Threshold: In logistic regression, the model predicts the probability of the positive class (e.g., \"churned\" in a churn prediction \n",
    "# problem). To obtain binary predictions, a classification threshold is applied. If the predicted probability is above the threshold, the instance \n",
    "# is classified as positive; otherwise, it is classified as negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f18f4b-816a-4da8-adef-db58605a4cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPR and FPR: The true positive rate (TPR), also known as sensitivity or recall, represents the proportion of positive instances correctly \n",
    "# classified as positive. It is calculated as TP / (TP + FN), where TP is the number of true positive instances and FN is the number of false \n",
    "# negative instances. The false positive rate (FPR) is the proportion of negative instances incorrectly classified as positive and is calculated as \n",
    "# FP / (FP + TN), where FP is the number of false positive instances, and TN is the number of true negative instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff62281-babb-4f5d-8723-8ecb61faf018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve: The ROC curve is created by plotting the TPR on the y-axis against the FPR on the x-axis at different classification thresholds. \n",
    "# Each point on the curve represents a different threshold. The curve starts at the point (0, 0) representing a threshold that classifies all \n",
    "# instances as negative, and it ends at the point (1, 1) representing a threshold that classifies all instances as positive. The ideal classifier \n",
    "# would have an ROC curve that reaches the point (0, 1), indicating a high TPR and a low FPR for all thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1d0aa9-8c71-4daf-b8af-80b50787f594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area Under the Curve (AUC): The area under the ROC curve (AUC) is a single metric that summarizes the overall performance of the classifier. \n",
    "# It represents the probability that a randomly chosen positive instance will be ranked higher than a randomly chosen negative instance by the \n",
    "# classifier. An AUC of 0.5 indicates a random classifier, while an AUC of 1.0 represents a perfect classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c0587a-bca7-46d3-9ad9-14cc77cd4881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Evaluation: The ROC curve and AUC provide insights into the model's discrimination ability and its performance across different \n",
    "# classification thresholds. A higher AUC generally indicates better model performance, as it suggests a larger area under the curve and a better \n",
    "# trade-off between TPR and FPR. By examining the ROC curve, one can select the desired threshold that balances the model's sensitivity (TPR) and \n",
    "# specificity (1 - FPR) based on the specific requirements of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3418174f-3db5-4aef-a465-a2593509f7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In summary, the ROC curve and AUC are valuable tools for evaluating the performance of a logistic regression model, enabling a comprehensive\n",
    "#  analysis of its classification performance across various thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf26b242-a112-4905-b18a-fcef191a53dc",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9afade-bcd6-44b1-aaf9-05f155c9041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection techniques in logistic regression aim to identify and select the most relevant and informative features for building an optimal\n",
    "# model. By reducing the number of features, these techniques can improve the model's performance in several ways, including:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8486e1f6-1e9a-45e9-9c3f-15717285203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Reduced Overfitting: Including irrelevant or redundant features can lead to overfitting, where the model fits the training data too closely \n",
    "# and fails to generalize well to new data. Feature selection helps mitigate overfitting by focusing on the most informative features and reducing \n",
    "# noise and irrelevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ea9fdb-37a9-44e9-84ed-49ce0c2d6f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Improved Interpretability: Including a smaller subset of features makes the model more interpretable and easier to understand. By selecting \n",
    "# the most relevant features, you can identify the key variables that contribute significantly to the prediction, facilitating the interpretation \n",
    "# of the model's results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e388f8a-56ac-478a-9706-90caf29be2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Reduced Training Time and Complexity: By eliminating unnecessary features, feature selection reduces the dimensionality of the problem. \n",
    "# This, in turn, reduces the computational complexity and training time required to build the model, allowing for faster and more efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fab258-badc-480b-a246-64730f59fd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are some common techniques for feature selection in logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494e59c2-34fd-44db-aebf-0f69ba069fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Univariate Selection: This method involves selecting features based on their individual relationship with the target variable. Statistical \n",
    "# tests such as chi-square test, t-test, or ANOVA can be used to assess the significance of each feature independently. Features that exhibit high \n",
    "# statistical significance are selected for inclusion in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd241a3-273f-4b23-929e-f4fa49af0193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Recursive Feature Elimination (RFE): RFE is an iterative method that starts with all features and progressively removes the least important \n",
    "# ones. It trains the model on the full feature set, ranks the features based on their importance (using coefficients or feature weights), and \n",
    "# eliminates the least important feature. This process is repeated until a desired number of features or a specified stopping criterion is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca85c4a-7a42-4717-b8f7-ce2a3701fb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Regularization-Based Methods: Regularization techniques like L1 regularization (Lasso) and L2 regularization (Ridge) inherently perform \n",
    "# feature selection by penalizing large parameter values. These methods encourage sparse solutions by driving some parameter values to zero or \n",
    "# close to zero. The features corresponding to non-zero parameter values are considered important and selected for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21bbe38-854c-4365-a3db-c7b75dbe1c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Feature Importance from Tree-Based Models: Tree-based models like Random Forest or Gradient Boosting can provide a measure of feature \n",
    "# importance. Features that have a higher impact on the model's performance, such as those frequently used in splitting decisions, are considered \n",
    "# more important and selected for inclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643b0b71-ab11-4287-b609-5d815c2f3e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Correlation Analysis: Features that are highly correlated with the target variable are often good candidates for inclusion in the model.\n",
    "# Correlation analysis can identify such relationships and help in selecting features that exhibit a strong association with the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbec50bf-4f63-43ac-8baf-e8d1656dc84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's important to note that the choice of feature selection technique depends on the specific problem and the characteristics of the dataset. \n",
    "# It's recommended to experiment with multiple techniques and evaluate their impact on the model's performance using appropriate validation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05506da6-87df-456d-b1d6-0cecee34d4b8",
   "metadata": {},
   "source": [
    "# Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5affc01b-7cf2-4cbd-8586-c0fb4d2567c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling imbalanced datasets in logistic regression is an important consideration as it can lead to biased models that favor the majority class.\n",
    "# Here are some strategies for dealing with class imbalance in logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b849e5-9ab7-468f-b7b5-2fb4101d37f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Resampling: One common approach is to rebalance the dataset through data resampling techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98a7e0e-cf12-449d-a543-231fd7f24d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampling: Randomly removing instances from the majority class to match the number of instances in the minority class. This can help reduce \n",
    "# the dominance of the majority class and balance the class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06b058a-88ce-401d-a541-cea24b2428f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling: Creating synthetic instances for the minority class by replicating or generating new samples. This can help increase the \n",
    "# representation of the minority class and balance the class distribution. Techniques like SMOTE (Synthetic Minority Over-sampling Technique) are \n",
    "# commonly used for generating synthetic samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cdf550-2532-4a10-9391-0e14bfae5606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combination of Oversampling and Undersampling: Combining both undersampling and oversampling techniques to achieve a more balanced dataset. For \n",
    "# example, undersampling the majority class and then oversampling the minority class to obtain a more balanced representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d10bed-b6b0-4ff7-9c54-ab775b729caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Class Weighting: Adjusting the class weights during model training to give higher importance to the minority class. This can be achieved by \n",
    "# assigning higher weights to instances of the minority class in the cost function during model optimization. Logistic regression implementations \n",
    "# often provide options to specify class weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903e6f1a-2bb0-4375-bcc0-dc68ef7d69c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Threshold Adjustment: By default, logistic regression uses a classification threshold of 0.5. However, in imbalanced datasets, adjusting the \n",
    "# threshold can be beneficial. Depending on the problem and the trade-off between false positives and false negatives, you can lower the threshold \n",
    "# to increase sensitivity (TPR) or raise it to increase specificity (TNR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3401658b-820c-4f7e-920b-debb88fb133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Cost-Sensitive Learning: Modifying the cost function to explicitly incorporate the cost of misclassification for different classes. \n",
    "# This approach assigns higher penalties to misclassifications of the minority class, thereby encouraging the model to focus more on correctly \n",
    "# predicting the minority class instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6687462-821f-480c-8068-65647552969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Ensemble Methods: Utilizing ensemble methods like bagging, boosting (e.g., AdaBoost), or stacking can help improve the model's performance \n",
    "# on imbalanced datasets. These methods combine multiple models to create a stronger and more balanced classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8882231-64dc-413b-a805-086dcd73979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Anomaly Detection Techniques: If the imbalance is extreme and the minority class is considered an anomaly or rare event, anomaly detection \n",
    "# techniques like One-Class SVM or Isolation Forest can be applied to identify and classify instances of the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971ec794-662a-4b20-a2c8-9d5438e16ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's important to note that the choice of strategy depends on the specific dataset, problem domain, and available resources. It's recommended \n",
    "# to experiment with different techniques and evaluate their impact on the model's performance using appropriate evaluation metrics, such as \n",
    "# precision, recall, F1-score, or AUC-ROC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d854d18c-e301-48af-a1b7-003d3bf2de66",
   "metadata": {},
   "source": [
    "# Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f54213a-d73e-46b6-b984-2ebd58e3146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When implementing logistic regression, several issues and challenges may arise. One common challenge is multicollinearity among independent \n",
    "# variables, which refers to high correlation or interdependence between two or more predictors. Multicollinearity can cause problems in logistic \n",
    "# regression, such as unstable parameter estimates, difficulty in interpreting the effects of individual predictors, and increased standard errors. \n",
    "# Here are some strategies to address multicollinearity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c03367-bce1-410e-bac0-4e084c9f9be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Identify and assess multicollinearity: Use correlation matrices or variance inflation factor (VIF) to identify variables that exhibit high \n",
    "# correlation. VIF measures how much the variance of the estimated regression coefficients is increased due to multicollinearity. Generally, \n",
    "# variables with VIF values greater than 5 or 10 are considered highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bd2161-3fbf-4034-a431-bfa951e264fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Remove one of the correlated variables: If two or more variables are highly correlated, you can choose to remove one of them from the model. \n",
    "# Preferably, remove the variable that is theoretically less important or has less relevance to the outcome. This approach helps mitigate \n",
    "# multicollinearity by eliminating one source of high correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cd5e9b-3778-4d61-9ca4-47041f7bbd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature transformation: Instead of removing correlated variables, you can transform them to create new features. For example, you can create \n",
    "# interaction terms or polynomial terms to capture non-linear relationships between variables. By transforming variables, you can reduce the \n",
    "# correlation while preserving the information they contribute to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6da686-84bf-4eb2-8495-aabcb66fe538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Regularization techniques: Regularization methods like Ridge regression (L2 regularization) can help handle multicollinearity. Ridge regression \n",
    "# introduces a penalty term that shrinks the coefficients, reducing the impact of multicollinearity. The penalty term encourages the model to \n",
    "# distribute the weights more evenly across correlated variables, reducing their impact on the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdb6848-b2e6-4235-8139-de94d98fa54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Domain knowledge and data collection: Prior domain knowledge can provide insights into the variables' interrelationships and help in \n",
    "# identifying collinear variables. Collecting more data or including additional relevant variables can also help reduce multicollinearity. A larger \n",
    "# and more diverse dataset can provide a better representation of the relationships between variables, reducing the chance of high correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dadbe5-9c8d-40cb-a100-103a10d75c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Principal Component Analysis (PCA): PCA is a dimensionality reduction technique that can be applied when multicollinearity is present. \n",
    "# It transforms the original correlated variables into a new set of uncorrelated variables called principal components. The principal components \n",
    "# can be used as predictors in logistic regression, helping to address multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf3d08e-03a3-4cd6-8d0e-82b58d204468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's important to note that multicollinearity does not always need to be completely eliminated. The severity and impact of multicollinearity \n",
    "# depend on the specific problem and the context of the variables involved. Assessing the practical impact and implications of multicollinearity \n",
    "# on the model's performance is crucial before implementing any corrective measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55fd4af-8ea3-411a-a1b5-d69edeb067c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e75028-87d1-464b-bbeb-a7b707c7d7a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
